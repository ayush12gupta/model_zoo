# Pytorch Implementation of Conditional GAN 
## Usage
```bash
$ python3 main.py --ndata 'cifar10' --epochs 100
```
> **_NOTE:_** on Colab Notebook use following command:
```python
!git clone link-to-repo
%run main.py --ndata 'cifar10' --epochs 100 
```

## Contributed by:
* [Ayush Gupta](https://github.com/ayush12gupta)

## References

* **Title**: MoCoGAN: Decomposing Motion and Content for Video Generation
* **Authors**: Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, Jan Kautz
* **Link**: https://arxiv.org/pdf/1411.1784.pdf
* **Tags**: Neural Network, Generative Networks, GANs
* **Year**: 2017

# Summary 


## GANs

Generative adversarial nets were recently introduced as a novel way to train a generative model.
They consists of two ‘adversarial’ models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training
data rather than G. Both G and D could be a non-linear mapping function, such as a multi-layer perceptron.

## Motion And Content Decomposition GAN

In MoCoGAN, we assume a latent space of images Z<sub>I</sub>≡R<sup>d</sup> where each point z ∈ Z<sub>I</sub> represents an image, and a
video of K frames is represented by a path of length K in the latent space, [z<sup>(1)</sup>, ..., z<sup>(K)</sup>].  By adopting this formulation, videos of different lengths can be generated by paths of
different lengths. 
