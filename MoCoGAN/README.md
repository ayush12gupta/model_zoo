# Pytorch Implementation of MoCoGAN 
## Usage
We are using [this dataset](http://www.wisdom.weizmann.ac.il/%7Evision/SpaceTimeActions.html) which you need to extact and place all the files in data. 

We need to resize the dataset by following command:
```bash
$ python3 resize,py
```

```bash
$ python3 main.py --ndata 'cifar10' --epochs 100
```
> **_NOTE:_** on Colab Notebook use following command:
```python
!git clone link-to-repo
%run main.py --ndata 'cifar10' --epochs 100 
```

## Contributed by:
* [Ayush Gupta](https://github.com/ayush12gupta)

## References

* **Title**: MoCoGAN: Decomposing Motion and Content for Video Generation
* **Authors**: Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, Jan Kautz
* **Link**: https://arxiv.org/pdf/1411.1784.pdf
* **Tags**: Neural Network, Generative Networks, GANs
* **Year**: 2017

# Summary 


## GANs

Generative adversarial nets were recently introduced as a novel way to train a generative model.
They consists of two ‘adversarial’ models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training
data rather than G. Both G and D could be a non-linear mapping function, such as a multi-layer perceptron.

## Motion And Content Decomposition GAN

In MoCoGAN, we assume a latent space of images Z<sub>I</sub>≡R<sup>d</sup> where each point z ∈ Z<sub>I</sub> represents an image, and a
video of K frames is represented by a path of length K in the latent space, [z<sup>(1)</sup>, ..., z<sup>(K)</sup>].  By adopting this formulation, videos of different lengths can be generated by paths of
different lengths. We further assume that Z<sub>I</sub> is decomposed into the content Z<sub>C</sub> and motion Z<sub>M</sub> subspace.
The content subspace models motion-independent appearance in videos, while the motion subspace models motion-dependent appearance in videos.

## Framework

For a video, the content vector, zC, is sampled once and fixed. Then, a series of random variables[e<sup>(1)</sup>, ..., e<sup>(K)</sup>] is sampled and mapped to a series of motioncodes [z<sup>(1)</sup><sub>M</sub>,...,z<sup>(K)</sup><sub>M</sub>] via the recurrent neural network R<sub>M</sub>. We implement RM using a
one-layer GRU network. A generator GI produces a frame, x˜<sup>(k)</sup>, using the content and the motion vectors {z<sub>C</sub>, z<sup>(K)</sup><sub>M</sub> }. The discriminators, D<sub>I</sub> and D<sub>V</sub>, are trained on real and fake images and videos,
respectively, sampled from the training set v and the generated set v˜. The function S<sub>1</sub> samples a single frame from a
video, S<sub>T</sub> samples T consequtive frames. The framework can be seen

![img](https://github.com/ayush12gupta/model_zoo/blob/master/MoCoGAN/101574503_3226320640922153_3481496598897229824_n.jpg = 200x190)

## Implementation

We implement this model on Weizmann database.

## Results

| ![gif](https://github.com/ayush12gupta/model_zoo/blob/master/MoCoGAN/ezgif.com-video-to-gif.gif)| ![gif](https://github.com/ayush12gupta/model_zoo/blob/master/MoCoGAN/ezgif.com-video-to-gif%20(2).gif)| ![gif](https://github.com/ayush12gupta/model_zoo/blob/master/MoCoGAN/ezgif.com-video-to-gif%20(1).gif)| 
